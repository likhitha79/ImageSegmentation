{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Skymaps_segmentation_task_sneak_peek",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4FFkLi-pDF5"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "import torch\n",
        "import shutil\n",
        "import random\n",
        "import mlflow\n",
        "import IPython\n",
        "import numpy as np\n",
        "from getpass import getpass\n",
        "from glob import glob\n",
        "from skimage import color\n",
        "import albumentations as albu\n",
        "from google.colab import files\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageOps\n",
        "from torch.utils.data import DataLoader\n",
        "import segmentation_models_pytorch as smp\n",
        "from torch.utils.data import Dataset as BaseDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxKaPq5rpa3G"
      },
      "source": [
        "## UNet Model : EffNet-B0 + Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D0X6hFapRCQ"
      },
      "source": [
        "def seed_torch(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_torch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyUNVDjhpecq"
      },
      "source": [
        "train_images      = sorted(glob('/content/imgs/train/*'))\n",
        "validation_images = sorted(glob('/content/imgs/validation/*'))\n",
        "test_images       = sorted(glob('/content/imgs/test/*'))\n",
        "\n",
        "train_masks      = sorted(glob('/content/masks/train/*'))\n",
        "validation_masks = sorted(glob('/content/masks/validation/*'))\n",
        "test_masks       = sorted(glob('/content/masks/test/*'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9txxkRh4pgLx"
      },
      "source": [
        "def visualize(**images):\n",
        "    \"\"\"PLot images in one row.\"\"\"\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(' '.join(name.split('_')).title())\n",
        "        plt.imshow(image)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6X-LNI5pj8e"
      },
      "source": [
        "visualize(image=cv2.imread(train_images[0]), mask=cv2.imread(train_masks[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbMgkvxJpkno"
      },
      "source": [
        "class Dataset(BaseDataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            typ='train', \n",
        "            augmentation=None, \n",
        "            preprocessing=None,\n",
        "    ):\n",
        "        if typ == 'train':\n",
        "          self.images = train_images\n",
        "          self.masks = train_masks\n",
        "        elif typ == 'valid':\n",
        "          self.images = validation_images\n",
        "          self.masks = validation_masks\n",
        "        elif typ == 'test':\n",
        "          self.images = test_images\n",
        "          self.masks = test_masks\n",
        "\n",
        "        self.typ = typ\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # read data\n",
        "        image = np.array(ImageOps.autocontrast(Image.open(self.images[i]).convert('RGB')))\n",
        "        mask = np.array(Image.open(self.masks[i]).convert('L'))\n",
        "\n",
        "        bmask = np.array((mask == 0).astype(int))\n",
        "        # bmask = np.array((mask == 0).astype(int)) + np.array((mask == 39).astype(int))\n",
        "        pmask = np.array((mask == 39).astype(int))\n",
        "        wmask = np.array((mask == 78).astype(int))\n",
        "\n",
        "        # mask = np.stack([bmask, pmask, wmask], axis=-1).astype('float')\n",
        "        # mask = np.stack([bmask, wmask], axis=-1).astype('float')\n",
        "        mask = 0 * bmask + 1 * pmask + 2 * wmask\n",
        "        # apply augmentations\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        # apply preprocessing\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "            \n",
        "        return image / 255., mask\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "339RSkJ6pl0k"
      },
      "source": [
        "dataset = Dataset(typ='train')\n",
        "print(len(dataset))\n",
        "\n",
        "image, mask = dataset[1] # get some sample\n",
        "print(mask.shape)\n",
        "\n",
        "visualize(image=image, bg=mask == 0, pl=mask == 1, wd=mask == 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNHPRU-hpoGP"
      },
      "source": [
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "\n",
        "        albu.HorizontalFlip(p=0.5),\n",
        "\n",
        "        albu.ShiftScaleRotate(scale_limit=0.2, rotate_limit=90, shift_limit=0.3, p=0.3, border_mode=0),\n",
        "        albu.SmallestMaxSize(max_size=768, always_apply=True),\n",
        "        albu.GaussNoise(p=0.3),\n",
        "        albu.Perspective(p=0.3),\n",
        "\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.CLAHE(p=0.3),\n",
        "                albu.RandomBrightnessContrast(p=0.3),\n",
        "                albu.RandomGamma(p=0.3),\n",
        "                albu.HueSaturationValue(p=0.3)\n",
        "            ],\n",
        "            p=0.3,\n",
        "        ),\n",
        "\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.Sharpen(p=0.3),\n",
        "                albu.Blur(blur_limit=3, p=0.3),\n",
        "                albu.MotionBlur(blur_limit=3, p=0.3),\n",
        "            ],\n",
        "            p=0.3,\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    return albu.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    test_transform = [\n",
        "        albu.SmallestMaxSize(max_size=768, always_apply=True),\n",
        "    ]\n",
        "    return albu.Compose(test_transform)\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "def to_tensor_mask(x, **kwargs):\n",
        "    return x.astype('int32')\n",
        "\n",
        "def get_preprocessing(preprocessing_fn):\n",
        "    _transform = [\n",
        "        albu.Lambda(image=preprocessing_fn),\n",
        "        albu.Lambda(image=to_tensor, mask=to_tensor_mask),\n",
        "    ]\n",
        "    \n",
        "    return albu.Compose(_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq6tPMgtppUs"
      },
      "source": [
        "augmented_dataset = Dataset(\n",
        "    augmentation=get_training_augmentation() \n",
        ")\n",
        "\n",
        "for i in range(3):\n",
        "    image, mask = augmented_dataset[i]\n",
        "    visualize(image=image, bg=mask == 0, pl=mask == 1, wd=mask == 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZqFrW6hfv-V"
      },
      "source": [
        "ENCODER = 'timm-efficientnet-b0' # 'resnet34' \n",
        "ENCODER_WEIGHTS = 'noisy-student' # 'imagenet'  \n",
        "ACTIVATION = None \n",
        "DEVICE = 'cuda'\n",
        "\n",
        "model = smp.Unet(\n",
        "    encoder_name=ENCODER,\n",
        "    encoder_weights=ENCODER_WEIGHTS, \n",
        "    classes=3,\n",
        "    activation=ACTIVATION,\n",
        "    decoder_attention_type='scse'\n",
        ")\n",
        "\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
        "\n",
        "train_dataset = Dataset(\n",
        "    typ = 'train',\n",
        "    augmentation=get_training_augmentation(), \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        ")\n",
        "\n",
        "valid_dataset = Dataset(\n",
        "    typ = 'valid',\n",
        "    augmentation=get_validation_augmentation(), \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e0rssW2r5k5"
      },
      "source": [
        "class IoU(torch.nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(IoU, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        \n",
        "        # print(np.unique(inputs.cpu().numpy().flatten()))\n",
        "        # print(np.unique(targets.cpu().numpy().flatten()))\n",
        "\n",
        "        # inputs = F.sigmoid(inputs)   \n",
        "\n",
        "        inputs = inputs.long()\n",
        "        targets = targets.long()\n",
        "\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        intersection = (inputs * targets).sum()\n",
        "        total = (inputs + targets).sum()\n",
        "        union = total - intersection \n",
        "        \n",
        "        IoU = (intersection + smooth)/(union + smooth)\n",
        "                \n",
        "        return IoU\n",
        "\n",
        "# lloss = IoULoss()\n",
        "lloss = torch.nn.CrossEntropyLoss(weight=torch.tensor([0.1, 0.2, 0.7]).to('cuda'))\n",
        "\n",
        "metrics = [\n",
        "    IoU()\n",
        "]\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz7VH0c5f1OB"
      },
      "source": [
        "def show(x, y, pred):\n",
        "    image, gt_mask = x[0], y[0]\n",
        "    pr_mask = pred[0]\n",
        "\n",
        "    visualize(\n",
        "        image=np.moveaxis(image * 255, 0, -1),\n",
        "        bg_gt=np.moveaxis(gt_mask, 0, -1)[:, :, 0],\n",
        "        pl_gt=np.moveaxis(gt_mask, 0, -1)[:, :, 1],\n",
        "        wd_gt=np.moveaxis(gt_mask, 0, -1)[:, :, 2],\n",
        "        bg_mask=np.moveaxis(pr_mask, 0, -1)[:, :, 0], \n",
        "        pl_mask=np.moveaxis(pr_mask, 0, -1)[:, :, 1],\n",
        "        wd_mask=np.moveaxis(pr_mask, 0, -1)[:, :, 2],\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqZNGGH5sDr_"
      },
      "source": [
        "Train the model and save it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMuBv_BnsA_F"
      },
      "source": [
        "max_score = 1000\n",
        "device = 'cuda'\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "for i in range(40):\n",
        "    \n",
        "    print('\\nEpoch: {}'.format(i))\n",
        "\n",
        "    running_loss = []\n",
        "    \n",
        "    running_plant_iou  = []\n",
        "    running_weed_iou = []\n",
        "    running_bg_iou = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    with tqdm(train_dataloader) as iterator:\n",
        "            for x, y in iterator:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                pred = model(x)\n",
        "                loss = lloss(pred, y.long())\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                pred = torch.nn.Softmax(dim=1)(pred)\n",
        "                pred = torch.argmax(pred, dim=1)\n",
        "                for iter in range(len(pred)):\n",
        "                  # print(pred[iter].shape)\n",
        "                  # print(y[iter].shape)\n",
        "                  met1  = metrics[0](pred[iter] == 0, y[iter] == 0)\n",
        "                  met2  = metrics[0](pred[iter] == 1, y[iter] == 1)\n",
        "                  met3  = metrics[0](pred[iter] == 2, y[iter] == 2)\n",
        "\n",
        "                  running_bg_iou.append(met1.item())\n",
        "                  running_plant_iou.append(met2.item())\n",
        "                  running_weed_iou.append(met3.item())\n",
        "\n",
        "                del pred, x, y\n",
        "\n",
        "                running_loss.append(loss.item())\n",
        "\n",
        "                iterator.set_postfix_str('Train Loss: ' + str(np.mean(np.array(running_loss))) + \\\n",
        "                                        ' Train BG IoU: ' + str(np.mean(np.array(running_bg_iou))) + \\\n",
        "                                         ' Train Plant IoU: ' + str(np.mean(np.array(running_plant_iou))) + \\\n",
        "                                         ' Train Weed IoU: ' + str(np.mean(np.array(running_weed_iou))))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    running_val_loss = []\n",
        "    running_val_plant_iou  = []\n",
        "    running_val_weed_iou = []\n",
        "    running_val_bg_iou = []\n",
        "\n",
        "    c = 0\n",
        "    with tqdm(valid_dataloader) as iterator:\n",
        "        for x, y in iterator:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "              x, y = x.to(device), y.to(device)\n",
        "              pred = model(x)\n",
        "              loss = lloss(pred, y.long())\n",
        "\n",
        "              pred = torch.nn.Softmax(dim=1)(pred)\n",
        "              pred = torch.argmax(pred, dim=1)\n",
        "              for iter in range(len(pred)):\n",
        "                \n",
        "                met1  = metrics[0](pred[iter] == 0, y[iter] == 0)\n",
        "                met2  = metrics[0](pred[iter] == 1, y[iter] == 1)\n",
        "                met3  = metrics[0](pred[iter] == 2, y[iter] == 2)\n",
        "\n",
        "                running_val_bg_iou.append(met1.item())\n",
        "                running_val_plant_iou.append(met2.item())\n",
        "                running_val_weed_iou.append(met3.item())\n",
        "\n",
        "            if c == 1:\n",
        "               show(x.detach().cpu().numpy(), y.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
        "\n",
        "            del pred, x, y\n",
        "\n",
        "            running_val_loss.append(loss.item())\n",
        "\n",
        "            iterator.set_postfix_str('Valid Loss: ' + str(np.mean(np.array(running_val_loss))) + \\\n",
        "                                    ' Valid BG IoU: ' + str(np.mean(np.array(running_val_bg_iou))) + \\\n",
        "                                      ' Valid Plant IoU: ' + str(np.mean(np.array(running_val_plant_iou))) + \\\n",
        "                                      ' Valid Weed IoU: ' + str(np.mean(np.array(running_val_weed_iou))))\n",
        "            \n",
        "            c += 1\n",
        "\n",
        "    if max_score > np.mean(np.array(running_val_loss)):\n",
        "        max_score = np.mean(np.array(running_val_loss))\n",
        "        torch.save(model, './best_model.pth')\n",
        "        print('Model saved!')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}